{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from peft import PeftModel    \n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"EleutherAI/pythia-160m\"\n",
    "adapters_list = [\"persuasion-scaling-laws/pythia-160m\"] \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load prompts\n",
    "df = pd.read_csv('prompts.csv')\n",
    "\n",
    "# Initialize columns\n",
    "df['prompt_with_chat_template'] = ''  \n",
    "df['response'] = ''\n",
    "df['temperature'] = 1  \n",
    "df['top_k'] = 20  \n",
    "df['top_p'] = 0.9  \n",
    "\n",
    "all_dfs = []  # List to store dataframes for each adapter\n",
    "\n",
    "for adapters_name in adapters_list:\n",
    "    m = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map={\"\": 0}\n",
    "    )\n",
    "\n",
    "    m = PeftModel.from_pretrained(m, adapters_name)\n",
    "    m = m.merge_and_unload()\n",
    "    tok = LlamaTokenizer.from_pretrained(model_name)\n",
    "    tok.bos_token_id = 1\n",
    "\n",
    "    stop_token_ids = [0]\n",
    "\n",
    "    m = m.to(device)\n",
    "    \n",
    "    # Extract the part of the adapter name after \"persuasion-scaling-laws/Llama-2-7b-hf-\"\n",
    "    adapter_short_name = adapters_name.split(\"persuasion-scaling-laws/pilot-Llama-2-7b-hf-\")[-1]\n",
    "\n",
    "    df['model'] = adapter_short_name\n",
    "\n",
    "    # Iterate over the \"prompt_full_text\" column\n",
    "    for index, row in df.iterrows():\n",
    "        prompt = row['prompt_full_text']\n",
    "        prompt_with_chat_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n ### Instruction:\\n\" + prompt + \"\\n\\n### Response:\\n\"\n",
    "        \n",
    "        inputs = tok(prompt_with_chat_template, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        outputs = m.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=600, temperature=row['temperature'], top_k=row['top_k'], top_p=row['top_p'])\n",
    "        response_text = tok.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "        # Split the response_text at \"### Response:\" and select the second part\n",
    "        response_only = response_text.split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "        # Save the response and the prompt with chat template in the dataframe\n",
    "        df.at[index, 'response'] = response_only\n",
    "        df.at[index, 'prompt_with_chat_template'] = prompt_with_chat_template\n",
    "\n",
    "    # Save the dataframe to a new CSV file\n",
    "    df.to_csv(f'completions/{adapter_short_name}_responses.csv', index=False, encoding='utf-8')\n",
    "    all_dfs.append(df.copy())  # Store the dataframe for later concatenation\n",
    "    \n",
    "    # Function to call the Hugging Face Inference API\n",
    "def query_hf_api(prompt, model, parameters):\n",
    "    headers = {\"Authorization\": f\"Bearer {'token here'}\"}\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/meta-llama/{model}\"\n",
    "    while True:\n",
    "        response = requests.post(API_URL, headers=headers, json={\"inputs\": prompt, \"parameters\": parameters})\n",
    "        data = response.json()\n",
    "        if 'error' in data and 'loading' in data['error']:\n",
    "            print('Model is loading, waiting...')\n",
    "            time.sleep(10)  # wait for 10 seconds before retrying\n",
    "        else:\n",
    "            return data[0]['generated_text'].strip()  # Extract the 'generated_text' value and rstrip leading and trailing whitespaces\n",
    "\n",
    "# After generating responses for each adapter\n",
    "hf_model = 'Llama-2-7b-chat-hf'\n",
    "hf_parameters = {\"max_new_tokens\": 600, \"temperature\": 1, \"return_full_text\": False}\n",
    "\n",
    "df_hf = df.copy()  # Create a copy of the dataframe for Hugging Face responses\n",
    "df_hf['model'] = hf_model\n",
    "\n",
    "for index, row in df_hf.iterrows():\n",
    "    prompt = row['prompt_full_text']\n",
    "    prompt_with_chat_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n ### Instruction:\\n\" + prompt + \"\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # Call the Hugging Face Inference API to generate a response\n",
    "    response_text = query_hf_api(prompt_with_chat_template, hf_model, hf_parameters)\n",
    "\n",
    "    # Split the response_text at \"### Response:\" and select the second part\n",
    "    response_only = response_text.split(\"### Response:\\n\")[-1].strip()\n",
    "\n",
    "    # Save the response and the prompt with chat template in the dataframe\n",
    "    df_hf.at[index, 'response'] = response_only\n",
    "    df_hf.at[index, 'prompt_with_chat_template'] = prompt_with_chat_template\n",
    "\n",
    "# Save the dataframe to a new CSV file\n",
    "df_hf.to_csv(f'completions/{hf_model}_responses.csv', index=False, encoding='utf-8')\n",
    "all_dfs.append(df_hf)  # Add the dataframe to the list\n",
    "\n",
    "# Concatenate all dataframes and save to a single CSV file\n",
    "final_df = pd.concat(all_dfs)\n",
    "\n",
    "# Add a new column 'response_id' with unique IDs for each row\n",
    "final_df['response_id'] = range(1, len(final_df) + 1)\n",
    "\n",
    "final_df.to_csv('completions/all_responses.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobi.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
