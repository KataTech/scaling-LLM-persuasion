model,attrition_count,total,proportion
gpt4-turbo,20,2491,0.8028904054596547
qwen-72b,20,2467,0.8107012565869477
llama2-70b,20,2447,0.8173273395995097
claude3-opus,19,2431,0.781571369806664
pythia-12b,9,826,1.0895883777239708
pythia-70m,8,368,2.1739130434782608
yi-34b,7,793,0.8827238335435058
llama2-13b,7,826,0.847457627118644
qwen-14b,7,826,0.847457627118644
yi-9b,5,812,0.6157635467980296
falcon-7b,5,366,1.366120218579235
pythia-1.4b,5,351,1.4245014245014245
qwen1.5-500m,5,355,1.4084507042253522
pythia-6.9b,4,333,1.2012012012012012
falcon-40b,4,821,0.48721071863580995
qwen1.5-1.8b,4,347,1.1527377521613833
pythia-2.8b,3,330,0.9090909090909091
pythia-410m,3,370,0.8108108108108109
qwen1.5-4b,3,332,0.9036144578313252
pythia-1b,2,350,0.5714285714285714
llama2-7b,2,357,0.5602240896358543
yi-6b,2,350,0.5714285714285714
qwen1.5-7b,1,363,0.27548209366391185
pythia-160m,1,383,0.26109660574412535
