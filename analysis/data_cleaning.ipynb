{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import pyreadr # for reading R data files, which some of the data are in.\n",
    "\n",
    "SEED = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dcf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_info(df, name):\n",
    "    print(f\"{name}\")\n",
    "    print(f\"shape: {df.shape}\")\n",
    "    print(f\"columns: {df.columns}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855aa456",
   "metadata": {},
   "source": [
    "# 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a823f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_DIR = 'main_study/code/analysis/'\n",
    "ANNOTATION_DIR = 'main_study/data/annotation_materials/'\n",
    "COMPLETIONS_DIR = 'main_study/data/completions/'\n",
    "PROCESSED_DIR = 'main_study/code/analysis/output/processed_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4089c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### annotations\n",
    "completed_annotations_final = pd.read_csv(ANNOTATION_DIR + 'completed_annotations_final.csv')\n",
    "gpt_legibility = pd.read_csv(ANNOTATION_DIR + 'gpt-legibility_on-topic_valence_scores.csv')\n",
    "sample_for_annotation = pd.read_csv(ANNOTATION_DIR + 'sample_for_annotation.csv')\n",
    "\n",
    "#### completions \n",
    "falcon_40b = pd.read_csv(COMPLETIONS_DIR + 'falcon-40b_responses.csv')\n",
    "all_responses_combined = pd.read_csv(COMPLETIONS_DIR + 'all_responses_combined.csv')\n",
    "all_responses = pd.read_csv(COMPLETIONS_DIR + 'all_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d29b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## processed data \n",
    "def import_rdata(file_path):\n",
    "    result = pyreadr.read_r(file_path)\n",
    "    return result[None]  # Extract the DataFrame from the dictionary\n",
    "\n",
    "prepared_data_df = import_rdata(PROCESSED_DIR + 'prepared_data.rds')\n",
    "df_estimates_df = import_rdata(PROCESSED_DIR + 'df_estimates.rds')\n",
    "ate_df = import_rdata(PROCESSED_DIR + 'raw_model_ATEs.rds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_with_metrics_df = pd.read_csv(ANALYSIS_DIR + 'final_data_with_metrics.csv')\n",
    "prompts_df = pd.read_csv(ANALYSIS_DIR + 'prompts.csv')\n",
    "raw_data_final = pd.read_csv(ANALYSIS_DIR + 'raw_data_final.csv', skiprows=[1,2]) # the second and third rows are just extra headers that are not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d5162",
   "metadata": {},
   "source": [
    "# 2. Text Completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0e59f",
   "metadata": {},
   "source": [
    "### Completed annotations dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d004858",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_annotations_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f821d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in completed_annotations_final.response[0:5]: \n",
    "    print(res)\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(completed_annotations_final, \"completed_annotations_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaad0a3",
   "metadata": {},
   "source": [
    "### GPT legibility dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34739d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_legibility.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf3942",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(gpt_legibility, \"gpt_legibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5bf07",
   "metadata": {},
   "source": [
    "What are the columns `treatement_partisanship` and `issue_stance_valence` referring to? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e9879",
   "metadata": {},
   "source": [
    "### What are the `issues` that are involed here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d491c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt_legibility['issue_stance_full'].unique(), \"\\nTotal number of issue areas: \", gpt_legibility['issue_short'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ed0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt_legibility['issue_short'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt_legibility['issue_area'].unique(), \"\\nTotal number of issues: \", gpt_legibility['issue_area'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148c898",
   "metadata": {},
   "source": [
    "### What does subsetting the dataframe to a certain issue topic and partisanship look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "political_stance = 'conservative-coded'\n",
    "issue = 'The U.S. should make it a requirement that people work in order to receive Medicaid'\n",
    "subset = gpt_legibility[(gpt_legibility['issue_stance_full'] == issue) & (gpt_legibility['treatment_partisanship'] == political_stance)]\n",
    "count = 5\n",
    "for res in subset['response'][:count]: \n",
    "    print(res)\n",
    "    print(\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6804d",
   "metadata": {},
   "source": [
    "Well, this first response is a bit non-sensical.. But the rest of the messaging seem interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7bfbd",
   "metadata": {},
   "source": [
    "### Sample_for_annotation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_for_annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_for_annotation.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(sample_for_annotation, \"sample_for_annotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_for_annotation['Unnamed: 0'].max()) # I wonder if this is related to the 720 responses they mentioned, but the number is slightly off. \n",
    "# update: 720 of these are AI-generated. 10 are human. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04415f41",
   "metadata": {},
   "source": [
    "### Completions datasetS (yes, there are multiple -- one per model and two aggregate ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878aefb",
   "metadata": {},
   "source": [
    "Why does the `all_responses_combined` dataset have more rows? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b640139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(all_responses, \"all_responses\")\n",
    "print_df_info(all_responses_combined, \"all_responses_combined\")\n",
    "print_df_info(falcon_40b, \"falcon_40b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_responses_combined['model'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48263b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_responses = all_responses_combined[all_responses_combined['model'] == 'human']\n",
    "human_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d174152",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_responses[[\"issue_short\", \"issue_stance_valence\", \"treatment_partisanship\", \"response\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_responses(df, line_length=200, show_model=False):\n",
    "    for i, res in enumerate(df['response']):\n",
    "        print(f\"Response {i+1}\")\n",
    "        print(f\"Issue: {df['issue_stance_full'].values[i]}\")\n",
    "        print(f\"Partisanship: {df['treatment_partisanship'].values[i]}\")\n",
    "        print(f\"Issue Stance Valence: {df['issue_stance_valence'].values[i]}\")\n",
    "        if show_model:\n",
    "            print(f\"Model: {df['model'].values[i]}\")\n",
    "        print(\"-\" * max(5, int(line_length / 10)))\n",
    "        res = textwrap.fill(res, width=line_length)\n",
    "        print(res)\n",
    "        print(\"-\" * line_length)\n",
    "\n",
    "show_responses(human_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fccc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "samples = 10\n",
    "ai_samples = all_responses_combined[all_responses_combined['model'] != 'human'].sample(samples)\n",
    "show_responses(ai_samples, show_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6df0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c8f620d",
   "metadata": {},
   "source": [
    "# 3. Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(prepared_data_df, \"prepared_data_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(ate_df, \"ate_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef17967",
   "metadata": {},
   "outputs": [],
   "source": [
    "ate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2853d852",
   "metadata": {},
   "source": [
    "# 4. Persuasion and Demographic Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(final_data_with_metrics_df, \"final_data_with_metrics_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b8dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in final_data_with_metrics_df.columns: \n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(raw_data_final, \"raw_data_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_cols = [\"age_1\", \"education\", \"gender\", \"party_affiliation\", \"ideo_affiliation\", \"political_knowledge1\", \"political_knowledge2\", \"political knowledge3\"]\n",
    "exp_cols = [\"condition_assignment\", \"issue\", \"condition\", \"bin_size\", \"model\", \"variant\"]\n",
    "\n",
    "temp_df = raw_data_final[demographic_cols + exp_cols]\n",
    "temp_df.to_csv(\"raw_data_processed.csv\")\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f9fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = ['medicaid-1_1', 'medicaid-2_1', 'medicaid-3_1',\n",
    "       'medicaid-4_1', 'veterans-1_1', 'veterans-2_1', 'veterans-3_1',\n",
    "       'veterans-4_1', 'pensions-1_1', 'pensions-2_1', 'pensions-3_1',\n",
    "       'pensions-4_1', 'foreign_aid-1_1', 'foreign_aid-2_1', 'foreign_aid-3_1',\n",
    "       'foreign_aid-4_1', 'confinement-1_1', 'confinement-2_1',\n",
    "       'confinement-3_1', 'confinement-4_1', 'suicide-1_1', 'suicide-2_1',\n",
    "       'suicide-3_1', 'suicide-4_1', 'border-1_1', 'border-2_1', 'border-3_1',\n",
    "       'border-4_1', 'felon_voting-1_1', 'felon_voting-2_1',\n",
    "       'felon_voting-3_1', 'felon_voting-4_1', 'affirmative_action-1_1',\n",
    "       'affirmative_action-2_1', 'affirmative_action-3_1',\n",
    "       'affirmative_action-4_1', 'electoral_college-1_1',\n",
    "       'electoral_college-2_1', 'electoral_college-3_1',\n",
    "       'electoral_college-4_1']\n",
    "assignments_df = raw_data_final[assignments]\n",
    "assignments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_df = assignments_df.fillna(0)\n",
    "assignments_df['sum_effects'] = assignments_df.sum(axis=1)\n",
    "assignments_df['sum_effects'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae244a8c",
   "metadata": {},
   "source": [
    "Whais is the difference between `party_affiliation` and `ideo_affiliation`? And where is the measure of `persuasiveness`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_df_info(prompts_df, \"prompts_df\") # 30 is the number of messages generated per model, so presumably each model is prompted 30 times using these prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c802f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_df['prompt_full_text'].unique()[:5] # its just a repeat of the 3 template messages that they mentioend in \"message generation\" subsection of paper (pg. 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in final_data_with_metrics_df.columns: \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_cols = [\"age\", \"education\", \"gender\", \"party_affiliation\", \"ideo_affiliation\", \"political_knowledge\"]\n",
    "exp_cols = [\"issue\", \"issue_full\", \"treatment_message_id\", \"treatment_message\", \"condition\", \"model\", \"dv_response_mean\"]\n",
    "mediator_cols = [\"treatment_message_word_count\"]\n",
    "\n",
    "temp_df = final_data_with_metrics_df[demographic_cols + exp_cols + mediator_cols]\n",
    "temp_df.to_csv(\"final_data_processed.csv\")\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f33b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1832cce",
   "metadata": {},
   "source": [
    "# 5. Saving Useful Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_responses_combined[['issue_stance_full','response']].to_csv(\"llm_responses.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
